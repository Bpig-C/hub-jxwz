| <b>特性维度</b> | <b>RNN</b> | <b>LSTM</b> | <b>GRU</b> | <b>Transformer</b> |
| --- | --- | --- | --- | --- |
| <b>准确率</b> | 75-85% | 88-95% | 85-92% | 92-98% |
| <b>训练速度</b> | 最快 | 慢 | 中等 | 最慢 |
| <b>长序列处理</b> | 差 | 良好 | 较好 | 优秀 |
| <b>上下文理解</b> | 弱 | 强 | 中强 | 最强 |
| <b>多意图识别</b> | 有限 | 良好 | 较好 | 优秀 |
| <b>数据需求</b> | 少 | 中等 | 中等 | 大量 |
RNN：
    优点：内部结构简单, 对计算资源要求低，在短序列任务上性能和效果都表现优异。
    缺点：过长的序列导致梯度的计算异常, 容易发生梯度消失或爆炸。
    适合场景：车载系统的简单指令识别、实时性要求高的场景。

LSTM:
    优点：能够有效捕捉长序列之间的语义关联，缓解梯度消失或爆炸现象，结构相比RNN更复杂，添加了四个门控单元（遗忘门、输入门、细胞状态、输出门）。
    缺点：内部结构相对较复杂，训练速度比较慢。
    适用场景：文本生成、机器翻译等，适合处理数据存在长期依赖的问题。

GRU：
    优点：能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象，结构相比LSTM更简单，只有两个门控单元（更新门、重置门）
    缺点：不能完全解决梯度消失问题，不可并行计算
        适用场景：基本LSTM能做的事情GRU都能做，一般优先选择GRU


Transformer：
    优点：自注意力机制能捕捉全局依赖关系，并行处理数据，训练效率高，
    缺点：数据需求量大，计算开销大，数据推理延迟高
    适用场景：规模大、数据量大的场景

