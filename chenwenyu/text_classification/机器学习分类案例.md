# 案例一 植物分类模型
## 数据集与数据预处理

### `iris`植物分类数据集
```python
data = datasets.load_iris()
x, y = data.data, data.target
```
### data的主要内容示例：
```json
{'data': array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       ...             ...
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]]), 
'target': array([0, 0, 0,... 2, 2, 2]),
...
}
```
- 其中`data.data`为原始的特征表示，每一行一个样本，每个样本使用四个特征表示为向量。
- `data.target`则为样本对应的分类编号。
### 数据集拆分
```python
train_x, test_x, train_y, test_y = train_test_split(x, y, random_state=42)
```
- `random_state`为随机数种子，python本身实现的是伪随机。
- 伪随机过程的大致解释
	- **把一个大整数经过一次固定公式（线性同余或其他算法）变成另一个大整数，再把这个整数的一部分当成随机数，接着把结果再扔回公式，如此循环。**
	- 针对单一循环过程的解释：
		- 1. 保存一个内部数 `state`（初始值就是随机种子）。
		- 2. 每要一个随机数，执行一条固定公式：`state = (state × a + b) mod m`，其中 `a, b, m` 是写死的常数。
		- 3. 把新的 `state` 截成若干位，映射成 0~1 的浮点数，就是你要的“随机”值。
		- 4. 把 `state` 留下来，下次再喂回同一条公式。
	- **因为公式和常数永远不变，只要 `state`（种子）相同，整个序列就一模一样，所以是“伪”随机。**
## 机器学习模型训练和预测流程
- 模型定义（`model`）
- 模型训练（`.fit()`）
- 模型预测（`.predect(test)`）
- 结果返回（`prediction == test_y`）
### 线性模型（逻辑回归）
```python
linear_model.LogisticRegression(max_iter=1000)
model.fit(train_x, train_y)
prediction = model.predict(test_x)  
print("测试标签：", test_y)  
print("预测标签", prediction)
print("逻辑回归：", prediction == test_y)  
print("逻辑回归的预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等
```
```
LogisticRegression(max_iter=1000)
测试标签： [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1
 0]
预测标签 [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1
 0]
逻辑回归： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
逻辑回归的预测结果 38 38
```
### 决策树
```python
model = tree.DecisionTreeClassifier()  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("决策树：", prediction == test_y)  
print("决策树回归的预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等
```
```
决策树： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
决策树回归的预测结果 38 38
```

### 聚类(K-邻近算法)
- 注：K邻近算法的邻近点参数一般为奇数，否则当2n个邻近点距离两个分类一样近时难以区分会导致无法分类。
```python
model = neighbors.KNeighborsClassifier(n_neighbors=1)  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("KNN：", prediction == test_y)  
print("KNN-1预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等  
  
model = neighbors.KNeighborsClassifier(n_neighbors=3)  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("KNN：", prediction == test_y)  
print("KNN-3预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等  
  
model = neighbors.KNeighborsClassifier(n_neighbors=5)  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("KNN：", prediction == test_y)  
print("KNN-5预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等
```
```
KNN： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
KNN-1预测结果 38 38
KNN： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
KNN-3预测结果 38 38
KNN： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
KNN-5预测结果 38 38
```

# 案例二 中文文本分类

### 本地文本分类数据集示例：
```python
dataset = pd.read_csv("dataset.csv", sep="\t", header=None)  
print(dataset.head(5))
```
```bash
	   0                  1
0      还有双鸭山到淮阴的汽车票吗13号的   Travel-Query
1                从这里怎么回家         Travel-Query
2       随便播放一首专辑阁楼里的佛里的歌   Music-Play
3              给看一下墓王之王嘛        FilmTele-Play
4  我想看挑战两把s686打突变团竞的游戏视频   Video-Play
... 
```
### 文本预处理
#### `jieba`基本操作
- 使用jieba库处理进行分词并以空格间隔
- `lcut`和`cut`多了一个`list`转换操作
- lambda逐样本处理文本为sk-learn可学习的形式
```python
# jieba.cut()生成的是一个generator类的分词结果，需要使用list()进行转换，或者使用jieba.lcut  
# "_".join代表了使用_符合作为间隔符来将多个字符串进行合并  
input_sententce = dataset[0].apply(lambda x: " ".join(jieba.lcut(x)))  # sklearn对中文处理
```
#### `jieba`进阶操作
新词发现算法： 按照字之间的联合出现的频次，发现新的成语，修改词表
- 本质上是一种**在线的、基于字符级 HMM 序列标注的未登录词识别**，而不是离线挖掘新词的统计算法。

#### 字典型Vectorizer预处理
- 将分词后的结果按照词频记录为字典形式
```python
def pre_process_for_dict_vectorizer(dataset, column_index=0):  
    """为DictVectorizer和FeatureHasher准备的预处理"""  
    # 创建字典格式的特征：{词: 词频}  
    dict_data = []  
    for text in dataset[column_index]:  
        # 分词并统计词频  
        words = jieba.lcut(text)  
        word_counts = {}  
        for word in words:  
            # 过滤标点符号  
            if word.strip() and word not in ['，', '。', '！']:  
                word_counts[word] = word_counts.get(word, 0) + 1  
        dict_data.append(word_counts)  
    return dict_data
```
- 单条示例：
	- 原始数据（DataFrame格式）：还有双鸭山到淮阴的汽车票吗13号的
	- 字典型预处理后：{'13': 1, '到': 1, '双鸭山': 1, '号': 1, '吗': 1, '汽车票': 1, '淮阴': 1, '的': 2, '还有': 1}
### 特征提取
#### 1.`CountVectorizer`
特点
词袋模型（Bag of Words）：将文本转换为词频矩阵。
生成稀疏矩阵：每行代表文档，每列代表词项，值为词频。
支持预处理：可配置 stop_words（停用词）、ngram_range（N-gram）、max_features（最大特征数）等。

优缺点
✅ 直观易解释：特征直接对应原始词汇。
❌ 高维稀疏：词汇表大时内存消耗高（需存储所有词项）。
❌ 忽略词序和语义：纯统计词频。

适用场景
基线文本分类（如垃圾邮件检测）。
需要可解释性的场景（如关键词分析）。
```python
from sklearn.feature_extraction.text import CountVectorizer
corpus = ["This is a sentence.", "This is another sentence."]
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
print(vectorizer.get_feature_names_out())  # 输出: ['another', 'is', 'sentence', 'this']
```

#### 2.`TfidfVectorizer`
特点
TF-IDF 加权：在 CountVectorizer 基础上，用 TF-IDF（词频-逆文档频率）调整权重。
TF（Term Frequency）：词在当前文档的频率。
IDF（Inverse Document Frequency）：惩罚常见词（如 "the"、"is"）。

优缺点
✅ 降低常见词权重：突出文档特异性词。
❌ 仍需存储词汇表：内存问题与 CountVectorizer 类似。

适用场景
信息检索、相似文档匹配。
需要区分关键词重要性的任务（如新闻分类）。
```python
from sklearn.feature_extraction.text import TfidfVectorizer
corpus = ["Chinese luxury hotels turn to hawking food as guests tighten belts.", "Trump and Putin to meet in Alaska for high-stakes summit."]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(X.toarray())  # 输出 TF-IDF 加权矩阵
```

#### 3.`HashingVectorizer`
特点
哈希技巧（Hashing Trick）：通过哈希函数将词映射到固定维度的特征空间（无需存储词汇表）。
内存高效：适合大规模文本（如百万级文档）。

优缺点
✅ 低内存消耗：不存储词汇表，适合流式数据。
❌ 不可逆：无法回溯特征对应的原始词（牺牲可解释性）。
❌ 可能哈希冲突：不同词可能映射到同一特征（但概率低）。

适用场景
大规模在线学习（如实时日志分析）。
内存受限的分布式计算（如 Spark ML）。

```python
from sklearn.feature_extraction.text import HashingVectorizer
vectorizer = HashingVectorizer(n_features=10)
X = vectorizer.fit_transform(corpus)  # 输出稀疏矩阵（哈希值作为列）
```
#### 4.`DictVectorizer`
特点
处理字典格式输入：将 {特征名: 值} 的字典转换为数值矩阵。
独热编码：分类特征自动转为 one-hot 形式。

优缺点
✅ 兼容混合特征：支持文本、数值、分类特征混合输入。
❌ 不适合纯文本：需先手动提取特征（如词频字典）。

适用场景
结构化数据与文本结合的特征工程（如“标题+点击量”联合建模）。


```python
from sklearn.feature_extraction import DictVectorizer
data = [{"word": "apple", "count": 3}, {"word": "banana", "count": 2}]
vectorizer = DictVectorizer()
X = vectorizer.fit_transform(data)  # 输出: 'apple'和'banana'转为 one-hot，'count'保留原值
```
#### 5.`FeatureHasher`
特点
哈希技巧的通用版：类似 HashingVectorizer，但支持非文本输入（如字典）。
指定维度：通过 n_features 控制输出维度。

优缺点
✅ 内存高效：适合高基数分类特征（如用户ID）。
❌ 冲突风险：与 HashingVectorizer 相同。

适用场景
高维分类特征（如广告ID、用户标签）。
需要快速原型设计的场景。
```python
from sklearn.feature_extraction import FeatureHasher
data = [{"category": "A", "price": 10}, {"category": "B", "price": 20}]
hasher = FeatureHasher(n_features=4, input_type="dict")
X = hasher.transform(data)  # 输出哈希后的稀疏矩阵
```

#### 6.`文本特征提取工具对比表`

| 工具                 | 输入类型       | 输出类型      | 内存效率 | 可解释性 | 处理方式               | 适用场景                     | 主要优缺点                                                                 |
|----------------------|---------------|--------------|----------|----------|------------------------|----------------------------|---------------------------------------------------------------------------|
| `CountVectorizer`    | 文本列表       | 词频矩阵      | 低       | 高       | 词频统计               | 基线文本分类、关键词分析     | ✅ 直观易解释<br>❌ 高维稀疏，内存消耗大                                   |
| `TfidfVectorizer`    | 文本列表       | TF-IDF 矩阵   | 低       | 高       | 词频+逆文档频率加权     | 信息检索、文档相似度         | ✅ 突出重要词<br>❌ 仍需存储词汇表                                          |
| `HashingVectorizer`  | 文本列表       | 哈希矩阵      | **高**   | 无       | 哈希映射到固定维度      | 大规模流式数据、实时处理     | ✅ 内存高效<br>❌ 不可逆，可能哈希冲突                                     |
| `DictVectorizer`     | 字典列表       | 数值矩阵      | 中       | 高       | 独热编码+数值保留       | 结构化+文本混合特征          | ✅ 兼容混合特征<br>❌ 需手动预处理文本                                      |
| `FeatureHasher`      | 字典/文本列表  | 哈希矩阵      | **高**   | 无       | 通用哈希技巧            | 高维分类特征、内存受限场景   | ✅ 支持非文本输入<br>❌ 牺牲可解释性                                        |

##### 核心区别总结

1. **词袋模型**  
   - `CountVectorizer`：纯词频  
   - `TfidfVectorizer`：词频+重要性加权  

2. **哈希技巧**  
   - `HashingVectorizer`：专为文本设计  
   - `FeatureHasher`：通用型（支持字典输入）  

3. **结构化处理**  
   - `DictVectorizer`：自动处理分类/数值特征  

### 模型选取
#### 1.`KNeighbors`
原理：根据最近的k个邻居的类别投票决定分类。
特点：
无训练过程（惰性学习），但预测时计算量大。
适合小数据集，对异常值敏感。
```python
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=3)  # 选择3个邻居
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

```
#### 2.`SVM`
原理：找到最大化类别间隔的超平面。
特点：
高维数据表现好，但内存消耗高。
需调节C（正则化参数）和kernel。
```python
from sklearn.svm import SVC
model = SVC(kernel='linear')  # 线性核（也可用'rbf'）
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
#### 3.`DecisionTree`
原理：通过树形结构递归分割数据。
特点：
可解释性强（可用plot_tree可视化）。
容易过拟合（需剪枝或限制深度）。
```python
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth=5)  # 限制树深度防过拟合
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
#### 4.`MLP（Multi-Layer Perceptron`
原理：人工神经网络（全连接层）。
特点：
适合复杂非线性数据，但需要大量数据。
超参数多（层数、神经元数、学习率等）。
```python
model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)
model.fit(input_feature, targets_label)
```
#### 5.`RandomForest`
原理：多棵决策树集成投票。
特点：
抗过拟合，支持特征重要性评估。
比单棵决策树更鲁棒，但计算量更大。
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)  # 100棵树
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
#### 6.`OneVsRest`
原理：将多分类任务拆分为多个二分类。
特点：
兼容任何二分类模型（如Logistic Regression）。
类别不平衡时效果可能下降。
```python
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
model = OneVsRestClassifier(SVC())  # 以SVM为基模型
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```
#### 7.`模型总结对比`
##### 常见分类模型对比总结

| 模型                | 核心原理                     | 训练速度 | 预测速度 | 可解释性 | 特征需求              | 超参数敏感性 | 适用场景                     | 主要优缺点                                                                 |
|---------------------|-----------------------------|----------|----------|----------|-----------------------|--------------|----------------------------|---------------------------------------------------------------------------|
| **k-NN**            | 基于最近邻样本投票           | 快       | 慢       | 中       | 需特征缩放            | 低           | 小数据集、简单分类           | ✅ 无需训练<br>❌ 高维数据性能差，计算成本高                                |
| **SVM**             | 最大化分类间隔               | 慢       | 快       | 低       | 需特征缩放            | 高           | 高维数据、小样本分类         | ✅ 高维有效<br>❌ 内存消耗大，核函数选择敏感                               |
| **Decision Tree**   | 递归特征分割                 | 快       | 快       | **高**   | 无需特征缩放          | 中           | 可解释性优先、非线性数据     | ✅ 直观可视化<br>❌ 容易过拟合                                             |
| **Random Forest**   | 多决策树集成（Bagging）      | 中       | 中       | 中       | 无需特征缩放          | 低           | 通用分类、特征重要性分析     | ✅ 抗过拟合<br>❌ 内存占用较大                                            |
| **MLP**            | 多层神经网络                 | 慢       | 中       | 低       | 需特征缩放            | **高**       | 复杂模式、大规模数据         | ✅ 拟合能力强<br>❌ 需大量数据，调参复杂                                   |
| **OneVsRest**       | 多类分解为多个二分类         | 依赖基模型 | 依赖基模型 | 依赖基模型 | 依赖基模型           | 依赖基模型   | 多分类问题                   | ✅ 兼容二分类模型<br>❌ 类别不平衡时性能下降                                |

##### 关键特性对比

1. **线性 vs 非线性**
- **线性模型**：SVM（线性核）、Logistic Regression  
- **非线性模型**：Decision Tree、Random Forest、MLP、k-NN（近似非线性）  

2. **数据敏感性**
- **需特征缩放**：k-NN、SVM、MLP  
- **无需缩放**：Decision Tree、Random Forest  

3. **过拟合风险**
- **高风险**：Decision Tree（无剪枝时）、MLP（小数据时）  
- **稳健**：Random Forest、SVM（正则化强时）  

##### 选型建议

| 场景                          | 推荐模型               | 理由                                                                 |
|-------------------------------|-----------------------|----------------------------------------------------------------------|
| **高维小样本**                | SVM（线性核）         | 间隔最大化对小样本有效                                               |
| **需要可解释性**              | Decision Tree         | 可可视化决策路径                                                     |
| **大规模数据**                | Random Forest         | 并行训练高效，抗噪声                                                 |
| **多分类问题**                | OneVsRest + SVM/RF    | 灵活支持任意二分类基模型                                             |
| **计算资源有限**              | Logistic Regression   | 轻量级且速度快                                                       |
| **复杂模式（如图像/文本）**   | MLP                  | 神经网络适合学习深层特征                                             |

##### 模型选择捷径  
默认首试：RandomForest（平衡性能与易用性）。
需要解释：DecisionTree。
高维数据：SVM或Logistic Regression。
计算资源少：k-NN（小数据）或Linear SVM。

### 模型预测
#### 预测数据
```csv
帮我播放一下郭德纲的小品  
借我看看罗小黑战记嘛  
我想瞅牧神记的动画片
```
#### 预测代码
```python
def predict_model(input_feature, model):  
    """  
	:param input_feature: 向量化后的输入数据  
	:param model: 训练的模型  
	:return:  
	"""
    prediction = model.predict(input_feature)  
    probabilities = model.predict_proba(input_feature)  
    max_probability = np.max(probabilities, axis=1)  
  
    print("Sample predictions:", prediction[:5])  
    print("Sample max probabilities:", max_probability[:5])  
    return prediction, probabilities, max_probability
```
#### 文本分类模型预测结果比较

##### ​**​`CountVectorizer`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记             | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | ----------------------- | ------------------------ |
| KNeighbors   | ~~Music-Play (0.80)~~    | ~~Alarm-Update (0.20)~~ | ~~FilmTele-Play (0.40)~~ |
| SVM          | Audio-Play (0.70)        | FilmTele-Play (0.54)    | Video-Play (0.42)        |
| DecisionTree | ~~FilmTele-Play (0.14)~~ | FilmTele-Play (0.14)    | ~~FilmTele-Play (0.14)~~ |
| MLP          | Audio-Play (0.83)        | ~~Video-Play (0.79)~~   | Video-Play (1.00)        |
| RandomForest | ~~FilmTele-Play (0.32)~~ | FilmTele-Play (0.69)    | Video-Play (0.35)        |
| OneVsRest    | Audio-Play (0.74)        | FilmTele-Play (0.51)    | Video-Play (0.76)        |

---

#### ==​**​`TfidfVectorizer`组​**==​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记              | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | ------------------------ | ------------------------ |
| KNeighbors   | Audio-Play (0.20)        | FilmTele-Play (0.20)     | Video-Play (0.60)        |
| ==SVM==      | ==Audio-Play (0.85)==    | ==FilmTele-Play (0.64)== | ==Video-Play (0.89)==    |
| DecisionTree | ~~FilmTele-Play (0.14)~~ | FilmTele-Play (0.14)     | ~~FilmTele-Play (0.14)~~ |
| *MLP*        | *Video-Play (0.58)*      | ~~*Video-Play (1.00)*~~  | ~~*Video-Play (1.00)*~~  |
| RandomForest | ~~FilmTele-Play (0.37)~~ | FilmTele-Play (0.64)     | Video-Play (0.32)        |
| OneVsRest    | Audio-Play (0.78)        | FilmTele-Play (0.62)     | Video-Play (0.87)        |

---

#### ​**​`HashingVectorizer`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记          | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | -------------------- | ------------------------ |
| KNeighbors   | ~~FilmTele-Play (0.60)~~ | FilmTele-Play (0.40) | Video-Play (0.80)        |
| SVM          | Audio-Play (0.73)        | FilmTele-Play (0.63) | ~~Music-Play (0.19)~~    |
| DecisionTree | ~~FilmTele-Play (0.14)~~ | FilmTele-Play (0.14) | ~~FilmTele-Play (0.14)~~ |
| MLP          | Audio-Play (0.99)        | FilmTele-Play (0.81) | ~~Weather-Query (0.50)~~ |
| RandomForest | ~~FilmTele-Play (0.41)~~ | FilmTele-Play (0.76) | ~~Weather-Query (0.40)~~ |
| OneVsRest    | Audio-Play (0.70)        | FilmTele-Play (0.78) | ~~Radio-Listen (0.25)~~  |

---

#### ​**​`DictVectorizer`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记                      | 样本3: 看牧神记动画片                     |
| ------------ | ------------------------ | -------------------------------- | -------------------------------- |
| KNeighbors   | ~~Music-Play (0.40)~~    | ~~HomeAppliance-Control (0.80)~~ | ~~HomeAppliance-Control (0.40)~~ |
| SVM          | ~~Video-Play (0.68)~~    | FilmTele-Play (0.48)             | Video-Play (0.46)                |
| DecisionTree | ~~FilmTele-Play (0.15)~~ | FilmTele-Play (0.15)             | ~~FilmTele-Play (0.15)~~         |
| MLP          | ~~Video-Play (0.80)~~    | ~~Video-Play (0.28)~~            | Video-Play (0.74)                |
| RandomForest | ~~FilmTele-Play (0.30)~~ | FilmTele-Play (0.29)             | ~~FilmTele-Play (0.29)~~         |
| OneVsRest    | Audio-Play (0.50)        | FilmTele-Play (0.44)             | Video-Play (0.53)                |

---

#### ​**​`FeatureHasher`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记                      | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | -------------------------------- | ------------------------ |
| KNeighbors   | Radio-Listen (0.60)      | ~~HomeAppliance-Control (0.60)~~ | ~~Music-Play (0.60)~~    |
| SVM          | ~~Video-Play (0.72)~~    | FilmTele-Play (0.40)             | ~~FilmTele-Play (0.27)~~ |
| DecisionTree | ~~FilmTele-Play (0.15)~~ | FilmTele-Play (0.15)             | ~~FilmTele-Play (0.15)~~ |
| MLP          | ~~Video-Play (0.74)~~    | FilmTele-Play (0.54)             | ~~Music-Play (0.55)~~    |
| RandomForest | ~~FilmTele-Play (0.39)~~ | FilmTele-Play (0.23)             | ~~FilmTele-Play (0.34)~~ |
| OneVsRest    | Audio-Play (0.53)        | FilmTele-Play (0.79)             | ~~Music-Play (0.48)~~    |

---

#### 预测结果分析：

1. **特征提取**：`TfidfVectorizer`提取的特征中，有三种模型均预测正确（SVM/KNeighbors/OneVsRest），其中SVM置信度最高。
2. **最佳组合​**​：`TfidfVectorizer` + SVM 在样本1和样本3获得高置信度(0.85+0.89)，且分类合理
3. **最易预测的样本**：样本二，罗小黑战记。可能因为其与训练样本的电影单词结构相似。
4. **决策树模型​**：​ 在所有向量化组合中表现最差，预测结果单一（全是FilmTele-Play）且置信度低（0.14-0.15）
5. ​**​MLP模型​**​： 在`TfidfVectorizer`中样本3置信度达1.00（过拟合风险）
6. ​**​异常预测​**​：
	- HashingVectorizer组出现Weather-Query/Radio-Listen等明显错误分类
	- 郭德纲小品样本被预测为`Radio-Listen`似乎也相对合理，可能因为郭德纲本身相声偏多，又同时存在于音频和收音机当中。

