| 模型对比 | 正则表达式 | TF-IDF           | Bert  | LLMs  |
|------| ---------- |------------------|-------|-------|
| 精度   | 模式匹配精度极高，但语义理解能力为零。      | 词汇统计精度高，但语义理解能力弱 | 上下文语义理解能力极强。在理解类任务（如情感分析、问答）上达到接近人类的State-of-the-Art (SOTA) 性能。 | 通用语义理解与生成能力最强。具备强大的推理、概括和创造能力。但可能产生“幻觉”（编造看似合理但错误的内容）。 |
| 速度   | 速度极快       | 速度很快        | 速度较慢。需要GPU进行高效推理，计算复杂度高。Transformer的自注意力机制是计算密集型  | 速度非常慢。由于模型极其庞大，生成每个token都需要巨大计算量，通常需要多卡GPU或TPU集群。  |
| 成本   | 成本极低       | 成本很低        | 成本高。微调成本（需要GPU资源和标注数据）和推理成本（需要GPU实例部署）都显著较高  | 成本极高。训练成本是天文数字（数百万美元起），推理成本也非常高昂。通常通过API按使用量付费。  |
| 消耗   | 资源消耗可忽略不计。内存和CPU占用极少       | 资源消耗中等        | 资源消耗高。需要大量显存（GPU Memory）来加载模型进行计算  | 资源消耗极高。需要海量显存和计算单元，几乎无法在消费级硬件上本地运行最大模型。  |
| 数据需求 | 无需数据       | 需要大量无标注文本数据来计算逆文档频率（IDF）        | 需要大量无标注数据进行预训练，同时需要特定任务的标注数据进行微调  | 需要海量、跨领域的无标注文本数据进行预训练。指令微调需要高质量的指令-回答对数据。  |
| 可解释性 | 可解释性极强       | 可解释性强        | 可解释性差（黑盒）。难以理解模型内部为何做出某个决策，尽管有注意力权重可提供部分见解  | 可解释性极差（黑盒）。决策过程极其复杂，几乎无法追溯生成某个答案的原因。  |
