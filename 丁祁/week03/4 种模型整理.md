
### RNN、LSTM、GRU、Word2Ves 四种模型整理

- 序列模型： RNN、LSTM、GRU
  - 用于处理序列数据，如： 时间序列、文本句子， 核心是有记忆功能
- 词嵌入模型： Word2Ves
  - 用于将单词映射到稠密向量空间，是许多 NLP 任务的基础第一步

### RNN (Recurrent Neural Network) --循环神经网路
- 核心思想： 引入 ‘循环’ 结构，使网略具备短期记忆能力， 网络的当前输出不仅依赖与当前输入，还以来上一时刻的隐藏状态
- 优点：
  - 处理变长序列： 与传统神经网络不同，RNN 可以处理任意长度的序列输入，非常契合 语音、语言等任务
  - 参数共享： 在整个序列上共享同一组参数，大大减少了需要训练的参数量，降低了模型复杂度
  - 捕获时序依赖性：理论上 RNN 可以利用历史信息来影响当前决策，适合用于时间序列分析，语言模型等任务
- 缺点：
  - 梯度消失/梯度爆炸问题：RNN 最致命问题， 导致 RNN 难以学习长期依赖关系
    - 在反向传播通过时间（BPTT）时，梯度需要沿着时间步连续相乘，
      - 如果梯度大于1，多次连乘会梯度爆炸；
      - 如果梯度小于1，多次连乘会梯度消失；
  - 计算效率： 由于其顺序性（必须一步一步计算），无法利于现代 GPU 的并行计算优势，训练速度较慢
  - 实践中的记忆短暂：尽管理论上可以记住长期信息，但由于梯度消失问题，实际应用中RNN通常只能记住最近几步的信息
#### 小结：RNN是序列建模的奠基者，但其固有的梯度问题使其难以在实际中处理长序列，现在已较少单独使用。

### LSTM (Long Short-Term Memory) - 长短期记忆网络
- 核心思想：LSTM 是 RNN 的一种特殊变体，通过引入‘门控机制’来选择性的记住和忘记信息，从而解决 RNN 的长期依赖问题
- 关键结构（三个门）：
  - 遗忘门（forget gate）：决定从细胞状态中丢弃哪些信息
  - 输入门（input gate）： 决定哪些信息要存入细胞状态
  - 输出门（output gate）： 决定给予当前细胞状态输出什么
- 优点：
  - 有效解决梯度的消失问题，从而能够学习极长期的依赖关系
  - 强大的记忆控制：门控机制能自主决定记住重要信息，忘记无关信息，功能很强大
- 缺点：
  - 计算复杂：由于门控结构繁多，LSTM 的参数数量是普通 RNN 的4倍，计算量更大，训练更慢
  - 超参数较多：需要调优的参数比 RNN更多，模型也更复杂
  - 可能存在过拟合风险：因其强大的拟合能力，在数据量不足时容易过拟合
#### 小结： LSTM 是 RNN 的极大成功的改进，至今仍在许多序列任务中作为基准模型或核心模块被广泛使用。

### GRU (Gated Recurrent Unit) - 门控循环单元
- 核心思想：GRU是LSTM的一个变体，它简化了LSTM的结构，将遗忘门和输入门合并为一个“更新门”，并混合了细胞状态和隐藏状态。
- 关键结构（两个门）： 
  - 更新门 (Update Gate)：平衡过去隐藏状态和当前候选隐藏状态的重要性（类似于LSTM的遗忘门+输入门）。 
  - 重置门 (Reset Gate)：决定过去隐藏状态对当前候选隐藏状态的影响程度。
- 优点： 
  - 计算效率高：结构比LSTM更简单，参数更少，因此训练和预测速度更快。 
  - 性能相当：在许多任务上，GRU的表现与LSTM相当甚至更好，尤其是在数据集较小或序列不是极长时。 
  - 缓解过拟合：参数更少，一定程度上降低了过拟合的风险。
- 缺点： 
  - 表达能力的权衡：简化结构是一把双刃剑。对于非常复杂或需要精细控制记忆的序列任务，GRU的表达能力可能略逊于LSTM。 
  - 可解释性稍弱：将两个门合并后，其内部运作机制的清晰度不如LSTM。
#### 小结：GRU是LSTM的一个优秀、轻量化的替代方案，在实践中经常作为“首选”的RNN单元进行尝试，以在性能和效率之间取得平衡。

### Word2Vec
- 核心思想：Word2Vec不是一个序列模型，而是一种无监督学习技术。它通过训练一个浅层神经网络（只有一个隐藏层），来学习词汇的分布式表示（词向量）。
  - 其核心思想是“一个词的含义可以由它周围的词来定义”（分布式假设）
- 两种主要模型： 
  - CBOW (Continuous Bag-of-Words)：通过上下文词来预测中心词。训练速度快，对高频词效果更好。 
  - Skip-gram：通过中心词来预测上下文词。在小型数据集上表现更好，尤其能很好地处理低频词。
- 优点： 
  - 捕获语义和语法关系：学习到的词向量空间具有惊人的线性特性，例如：vector(“King”) - vector(“Man”) + vector(“Woman”) ≈ vector(“Queen”)。相似词在空间中距离相近。 
  - 维度稠密：生成的词向量是稠密向量（通常100-300维），相比于one-hot编码的稀疏高维向量，计算效率更高，且不会有维度灾难。 
  - 作为强大的特征提取器：训练好的词向量可以作为其他复杂NLP模型（如RNN、LSTM）的输入特征，极大地提升了文本分类、情感分析、机器翻译等任务的性能。它是NLP领域的基石技术。
- 缺点： 
  - 上下文无关：这是最大的局限性。Word2Vec为每个词生成一个唯一的、固定的向量表示，无法解决一词多义问题。例如，“apple”无论是表示水果还是公司，其向量都是相同的。 
  - 无法处理OOV (Out-of-Vocabulary)：无法为在训练词汇表中未出现过的词生成向量。 
  - 忽略词序：CBOW和Skip-gram模型在训练时都忽略了上下文词的顺序，将其视为一个集合（Bag-of-Words），这会损失一部分语言信息。
#### 小结：Word2Vec是词嵌入领域的开创性工作，极大地推动了NLP的发展。但其“静态词向量”的缺点也催生了后续如ELMo、BERT等“动态上下文词向量”模型的出现。

- 总结与对比
- 模型  	    主要目的	    核心优势	                    核心劣势	                适用场景
- RNN	        序列建模	    参数共享，处理变长序列	        梯度消失，难以学习长期依赖	基础序列模型，现已较少使用
- LSTM	    序列建模	    能有效学习极长期依赖，功能强大	计算复杂，参数多，训练慢	机器翻译、文本生成、语音识别等复杂长序列任务
- GRU	        序列建模	    计算高效，性能与LSTM相当	    对复杂模式表达能力可能稍弱	LSTM的轻量级替代，资源受限或序列不长时的首选
- Word2Vec	词嵌入	    捕获词汇语义关系，向量稠密高效	无法解决一词多义，上下文无关	作为任何NLP任务的预处理或输入特征

### 在实际的NLP系统中，这些模型通常是分层协作的：
- 第一步：使用Word2Vec（或更先进的如GloVe、FastText）在大规模语料上预训练词向量。每个单词被表示为一个稠密向量。
- 第二步：将这些词向量作为输入，送入LSTM或GRU这样的序列模型中。序列模型会考虑词的顺序和上下文信息，生成一个更能代表整个句子含义的向量。
- 第三步：将序列模型的输出用于最终的下游任务，如情感分类、命名实体识别等。