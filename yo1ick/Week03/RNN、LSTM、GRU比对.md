# RNN / LSTM / GRU 对比说明

## 1. RNN (Recurrent Neural Network)

### 计算过程：
- 每个时间步接收当前输入和上一个隐藏状态。
- 输出当前隐藏状态和（可选）输出。
- 公式：  
  \( h_t = \tanh(W_{xh} x_t + W_{hh} h_{t-1} + b_h) \)  
  \( y_t = W_{hy} h_t + b_y \)

### 优点：
- 结构简单，计算量小。
- 适合处理短序列任务。

### 缺点：
- 存在梯度消失/爆炸问题。
- 难以捕捉长期依赖。

---

## 2. LSTM (Long Short-Term Memory)

### 计算过程：
引入三个门控机制：
- 遗忘门：控制上一时刻记忆的保留程度。
- 输入门：控制当前输入的写入程度。
- 输出门：控制当前隐藏状态的输出程度。

### 优点：
- 能有效缓解梯度消失问题。
- 适合处理长序列和复杂依赖。

### 缺点：
- 参数较多，训练速度慢。
- 计算复杂度高。

---

## 3. GRU (Gated Recurrent Unit)

### 计算过程：
引入两个门控机制：
- 重置门：控制上一时刻隐藏状态对当前候选状态的影响。
- 更新门：控制隐藏状态的更新程度。

### 优点：
- 参数比 LSTM 少，训练更快。
- 在多数任务中表现与 LSTM 相当。

### 缺点：
- 在某些极端长序列任务上略逊于 LSTM。

---

## 总结对比

| 模型 | 门控数量 | 参数量 | 训练速度 | 长序列表现 |
|------|----------|--------|----------|------------|
| RNN  | 无       | 少     | 快       | 差         |
| LSTM | 3        | 多     | 慢       | 优         |
| GRU  | 2        | 中     | 中       | 良         |
