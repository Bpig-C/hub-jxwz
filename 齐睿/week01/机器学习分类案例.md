# 案例一 植物分类模型
## 数据集与数据预处理

### `iris`植物分类数据集
```python
data = datasets.load_iris()
x, y = data.data, data.target
```
### data的主要内容示例：
```json
{'data': array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       ...             ...
       [6.5, 3. , 5.2, 2. ],
       [6.2, 3.4, 5.4, 2.3],
       [5.9, 3. , 5.1, 1.8]]), 
'target': array([0, 0, 0,... 2, 2, 2]),
...
}
```
- 其中`data.data`为原始的特征表示，每一行一个样本，每个样本使用四个特征表示为向量。
- `data.target`则为样本对应的分类编号。
### 数据集拆分
```python
train_x, test_x, train_y, test_y = train_test_split(x, y, random_state=42)
```
- `random_state`为随机数种子，python本身实现的是伪随机。
- 伪随机过程的大致解释
	- **把一个大整数经过一次固定公式（线性同余或其他算法）变成另一个大整数，再把这个整数的一部分当成随机数，接着把结果再扔回公式，如此循环。**
	- 针对单一循环过程的解释：
		- 1. 保存一个内部数 `state`（初始值就是随机种子）。
		- 2. 每要一个随机数，执行一条固定公式：`state = (state × a + b) mod m`，其中 `a, b, m` 是写死的常数。
		- 3. 把新的 `state` 截成若干位，映射成 0~1 的浮点数，就是你要的“随机”值。
		- 4. 把 `state` 留下来，下次再喂回同一条公式。
	- **因为公式和常数永远不变，只要 `state`（种子）相同，整个序列就一模一样，所以是“伪”随机。**
## 机器学习模型训练和预测流程
- 模型定义（`model`）
- 模型训练（`.fit()`）
- 模型预测（`.predect(test)`）
- 结果返回（`prediction == test_y`）
### 线性模型（逻辑回归）
```python
linear_model.LogisticRegression(max_iter=1000)
model.fit(train_x, train_y)
prediction = model.predict(test_x)  
print("测试标签：", test_y)  
print("预测标签", prediction)
print("逻辑回归：", prediction == test_y)  
print("逻辑回归的预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等
```
```
LogisticRegression(max_iter=1000)
测试标签： [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1
 0]
预测标签 [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1
 0]
逻辑回归： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
逻辑回归的预测结果 38 38
```
### 决策树
```python
model = tree.DecisionTreeClassifier()  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("决策树：", prediction == test_y)  
print("决策树回归的预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等
```
```
决策树： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
决策树回归的预测结果 38 38
```

### 聚类(K-邻近算法)
- 注：K邻近算法的邻近点参数一般为奇数，否则当2n个邻近点距离两个分类一样近时难以区分会导致无法分类。
```python
model = neighbors.KNeighborsClassifier(n_neighbors=1)  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("KNN：", prediction == test_y)  
print("KNN-1预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等  
  
model = neighbors.KNeighborsClassifier(n_neighbors=3)  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("KNN：", prediction == test_y)  
print("KNN-3预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等  
  
model = neighbors.KNeighborsClassifier(n_neighbors=5)  
model.fit(train_x, train_y)  
prediction = model.predict(test_x)  
print("KNN：", prediction == test_y)  
print("KNN-5预测结果", (prediction == test_y).sum(), len(test_x))  # element wise equal 逐元素向量比较是否相等
```
```
KNN： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
KNN-1预测结果 38 38
KNN： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
KNN-3预测结果 38 38
KNN： [ True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True  True  True  True  True  True  True  True  True  True  True
  True  True]
KNN-5预测结果 38 38
```

# 案例二 中文文本分类

### 本地文本分类数据集示例：
```python
dataset = pd.read_csv("dataset.csv", sep="\t", header=None)  
print(dataset.head(5))
```
```bash
	   0                  1
0      还有双鸭山到淮阴的汽车票吗13号的   Travel-Query
1                从这里怎么回家         Travel-Query
2       随便播放一首专辑阁楼里的佛里的歌   Music-Play
3              给看一下墓王之王嘛        FilmTele-Play
4  我想看挑战两把s686打突变团竞的游戏视频   Video-Play
... 
```
### 文本预处理
#### `jieba`基本操作
- 使用jieba库处理进行分词并以空格间隔
- `lcut`和`cut`多了一个`list`转换操作
- lambda逐样本处理文本为sk-learn可学习的形式
```python
# jieba.cut()生成的是一个generator类的分词结果，需要使用list()进行转换，或者使用jieba.lcut  
# "_".join代表了使用_符合作为间隔符来将多个字符串进行合并  
input_sententce = dataset[0].apply(lambda x: " ".join(jieba.lcut(x)))  # sklearn对中文处理
```
#### `jieba`进阶操作
新词发现算法： 按照字之间的联合出现的频次，发现新的成语，修改词表
- 本质上是一种**在线的、基于字符级 HMM 序列标注的未登录词识别**，而不是离线挖掘新词的统计算法。

#### 字典型Vectorizer预处理
- 将分词后的结果按照词频记录为字典形式
```python
def pre_process_for_dict_vectorizer(dataset, column_index=0):  
    """为DictVectorizer和FeatureHasher准备的预处理"""  
    # 创建字典格式的特征：{词: 词频}  
    dict_data = []  
    for text in dataset[column_index]:  
        # 分词并统计词频  
        words = jieba.lcut(text)  
        word_counts = {}  
        for word in words:  
            # 过滤标点符号  
            if word.strip() and word not in ['，', '。', '！']:  
                word_counts[word] = word_counts.get(word, 0) + 1  
        dict_data.append(word_counts)  
    return dict_data
```
- 单条示例：
	- 原始数据（DataFrame格式）：还有双鸭山到淮阴的汽车票吗13号的
	- 字典型预处理后：{'13': 1, '到': 1, '双鸭山': 1, '号': 1, '吗': 1, '汽车票': 1, '淮阴': 1, '的': 2, '还有': 1}
### 特征提取
#### 1.`CountVectorizer`
```python
vector = CountVectorizer()  
vector.fit(input_sententce.values)   
input_feature = vector.transform(input_sententce.values)
```

#### 2.`TfidfVectorizer`
```python
vectorizer = TfidfVectorizer(max_features=max_features)  
input_feature = vectorizer.fit_transform(input_sentence.values)
```

#### 3.`HashingVectorizer`
```python
vectorizer = HashingVectorizer(n_features=max_features)  
input_feature = vectorizer.fit_transform(input_sentence.values)
```
#### 4.`DictVectorizer`
```python
vectorizer = DictVectorizer(sparse=True)  
input_feature = vectorizer.fit_transform(input_sentence)
```
#### 5.`FeatureHasher`
```python
vectorizer = FeatureHasher(n_features=max_features)  
input_feature = vectorizer.transform(input_sentence)
```
### 模型选取
#### 1.`KNeighbors`
```python
model = KNeighborsClassifier(n_neighbors=5)
model.fit(input_feature, targets_label)
```
#### 2.`SVM`
```python
model = SVC(kernel='linear', probability=True)
model.fit(input_feature, targets_label)
```
#### 3.`DecisionTree`
```python
model = DecisionTreeClassifier(max_depth=5)
model.fit(input_feature, targets_label)
```
#### 4.`MLP`
```python
model = MLPClassifier(hidden_layer_sizes=(100,), max_iter=500)
model.fit(input_feature, targets_label)
```
#### 5.`RandomForest`
```python
model = RandomForestClassifier(n_estimators=100)
model.fit(input_feature, targets_label)
```
#### 6.`OneVsRest`
```python
base_model = SVC(kernel='linear', probability=True)  
model = OneVsRestClassifier(base_model)
model.fit(input_feature, targets_label)
```
### 模型预测
#### 预测数据
```csv
帮我播放一下郭德纲的小品  
借我看看罗小黑战记嘛  
我想瞅牧神记的动画片
```
#### 预测代码
```python
def predict_model(input_feature, model):  
    """  
	:param input_feature: 向量化后的输入数据  
	:param model: 训练的模型  
	:return:  
	"""
    prediction = model.predict(input_feature)  
    probabilities = model.predict_proba(input_feature)  
    max_probability = np.max(probabilities, axis=1)  
  
    print("Sample predictions:", prediction[:5])  
    print("Sample max probabilities:", max_probability[:5])  
    return prediction, probabilities, max_probability
```
#### 文本分类模型预测结果比较

##### ​**​`CountVectorizer`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记             | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | ----------------------- | ------------------------ |
| KNeighbors   | ~~Music-Play (0.80)~~    | ~~Alarm-Update (0.20)~~ | ~~FilmTele-Play (0.40)~~ |
| SVM          | Audio-Play (0.70)        | FilmTele-Play (0.54)    | Video-Play (0.42)        |
| DecisionTree | ~~FilmTele-Play (0.14)~~ | FilmTele-Play (0.14)    | ~~FilmTele-Play (0.14)~~ |
| MLP          | Audio-Play (0.83)        | ~~Video-Play (0.79)~~   | Video-Play (1.00)        |
| RandomForest | ~~FilmTele-Play (0.32)~~ | FilmTele-Play (0.69)    | Video-Play (0.35)        |
| OneVsRest    | Audio-Play (0.74)        | FilmTele-Play (0.51)    | Video-Play (0.76)        |

---

#### ==​**​`TfidfVectorizer`组​**==​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记              | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | ------------------------ | ------------------------ |
| KNeighbors   | Audio-Play (0.20)        | FilmTele-Play (0.20)     | Video-Play (0.60)        |
| ==SVM==      | ==Audio-Play (0.85)==    | ==FilmTele-Play (0.64)== | ==Video-Play (0.89)==    |
| DecisionTree | ~~FilmTele-Play (0.14)~~ | FilmTele-Play (0.14)     | ~~FilmTele-Play (0.14)~~ |
| *MLP*        | *Video-Play (0.58)*      | ~~*Video-Play (1.00)*~~  | ~~*Video-Play (1.00)*~~  |
| RandomForest | ~~FilmTele-Play (0.37)~~ | FilmTele-Play (0.64)     | Video-Play (0.32)        |
| OneVsRest    | Audio-Play (0.78)        | FilmTele-Play (0.62)     | Video-Play (0.87)        |

---

#### ​**​`HashingVectorizer`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记          | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | -------------------- | ------------------------ |
| KNeighbors   | ~~FilmTele-Play (0.60)~~ | FilmTele-Play (0.40) | Video-Play (0.80)        |
| SVM          | Audio-Play (0.73)        | FilmTele-Play (0.63) | ~~Music-Play (0.19)~~    |
| DecisionTree | ~~FilmTele-Play (0.14)~~ | FilmTele-Play (0.14) | ~~FilmTele-Play (0.14)~~ |
| MLP          | Audio-Play (0.99)        | FilmTele-Play (0.81) | ~~Weather-Query (0.50)~~ |
| RandomForest | ~~FilmTele-Play (0.41)~~ | FilmTele-Play (0.76) | ~~Weather-Query (0.40)~~ |
| OneVsRest    | Audio-Play (0.70)        | FilmTele-Play (0.78) | ~~Radio-Listen (0.25)~~  |

---

#### ​**​`DictVectorizer`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记                      | 样本3: 看牧神记动画片                     |
| ------------ | ------------------------ | -------------------------------- | -------------------------------- |
| KNeighbors   | ~~Music-Play (0.40)~~    | ~~HomeAppliance-Control (0.80)~~ | ~~HomeAppliance-Control (0.40)~~ |
| SVM          | ~~Video-Play (0.68)~~    | FilmTele-Play (0.48)             | Video-Play (0.46)                |
| DecisionTree | ~~FilmTele-Play (0.15)~~ | FilmTele-Play (0.15)             | ~~FilmTele-Play (0.15)~~         |
| MLP          | ~~Video-Play (0.80)~~    | ~~Video-Play (0.28)~~            | Video-Play (0.74)                |
| RandomForest | ~~FilmTele-Play (0.30)~~ | FilmTele-Play (0.29)             | ~~FilmTele-Play (0.29)~~         |
| OneVsRest    | Audio-Play (0.50)        | FilmTele-Play (0.44)             | Video-Play (0.53)                |

---

#### ​**​`FeatureHasher`组​**​

| 模型类型         | 样本1: 播放郭德纲小品             | 样本2: 看罗小黑战记                      | 样本3: 看牧神记动画片             |
| ------------ | ------------------------ | -------------------------------- | ------------------------ |
| KNeighbors   | Radio-Listen (0.60)      | ~~HomeAppliance-Control (0.60)~~ | ~~Music-Play (0.60)~~    |
| SVM          | ~~Video-Play (0.72)~~    | FilmTele-Play (0.40)             | ~~FilmTele-Play (0.27)~~ |
| DecisionTree | ~~FilmTele-Play (0.15)~~ | FilmTele-Play (0.15)             | ~~FilmTele-Play (0.15)~~ |
| MLP          | ~~Video-Play (0.74)~~    | FilmTele-Play (0.54)             | ~~Music-Play (0.55)~~    |
| RandomForest | ~~FilmTele-Play (0.39)~~ | FilmTele-Play (0.23)             | ~~FilmTele-Play (0.34)~~ |
| OneVsRest    | Audio-Play (0.53)        | FilmTele-Play (0.79)             | ~~Music-Play (0.48)~~    |

---

#### 预测结果分析：

1. **特征提取**：`TfidfVectorizer`提取的特征中，有三种模型均预测正确（SVM/KNeighbors/OneVsRest），其中SVM置信度最高。
2. **最佳组合​**​：`TfidfVectorizer` + SVM 在样本1和样本3获得高置信度(0.85+0.89)，且分类合理
3. **最易预测的样本**：样本二，罗小黑战记。可能因为其与训练样本的电影单词结构相似。
4. **决策树模型​**：​ 在所有向量化组合中表现最差，预测结果单一（全是FilmTele-Play）且置信度低（0.14-0.15）
5. ​**​MLP模型​**​： 在`TfidfVectorizer`中样本3置信度达1.00（过拟合风险）
6. ​**​异常预测​**​：
	- HashingVectorizer组出现Weather-Query/Radio-Listen等明显错误分类
	- 郭德纲小品样本被预测为`Radio-Listen`似乎也相对合理，可能因为郭德纲本身相声偏多，又同时存在于音频和收音机当中。

